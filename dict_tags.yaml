# The 'tags' feature is a versatile method to tailor the bot's behavior for your use case.
# tags may be defined in this file for global effect, as well as in character cards and imgmodel definitions.

# Notes:
# - 'trigger' key is recommended for most tags (except 'global_tag_keys' and 'tag_preset_name').
# - Multiple comma-separated trigger phrases can be used per each tag (trigger: 'a boy,a dog' can match either 'a boy' or 'a dog')
# - You may simplify triggers using this syntax: 'a {boy|dog}' --- the script will read this as 'a boy,a dog'
# - The behavior of tags listed higher may override ones listed lower. Sources are also prioritized: character > imgmodel > base_tags > global_base_tags
# - Valid tag keys are detailed at the end of this file and on GitHub

# 'global_tag_keys' apply to ALL tag definitions. Keys defined in your tags will override global tag keys.
global_tag_keys:
  search_mode: userllm          # Search method for trigger matching. 'user' = user prompt only / 'llm' = bot reply only / 'userllm' = search both for matches
  case_sensitive: false         # If your triggers are case sensitive
  text_joining: ' '               # Relevant to 'insert_text'. Method of joining the inserted text to the user's llm prompt.
  img_text_joining: ', '          # Relevant to 'positive_prompt' (and '_prefix' / '_suffix'). Method of joining the inserted text to the img prompt.
  positive_prompt_method: after   # Relevant in tags with key 'positive_prompt'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text

# 'base_tags' are always in effect. Base tags have lower priority than tags defined anywhere else.
base_tags:
    # Use {time} or {date} written anywhere in your prompts, which will be replaced with the current date/time. The tags 'offset' and '(date/time)_format' will apply to the result.
  - time_offset: 0.0        # 0 = today's date (system time). -0.5 shifts the current date to be 12 hours ago. (UOM = days).
    time_format: '%H:%M:%S' # Examples: '%H:%M:%S' (HH:MM:SS), '%I:%M:%S %p' (12HR:MM:SS AM/PM)
    date_format: '%Y-%m-%d' # Examples: '%Y-%m-%d' (YYYY-MM-DD), '%A, %B %d, %Y' (DayName, MonthName DD, YYYY)

# Stable Diffusion payload and textgen-webui params can be randomly modified within specified ranges, or a list of strings (ex: sampler names), or from a boolean value. Must be formatted as a dictionary.
# These examples are commented out by default - you should uncomment and experiment :D
#  - img_param_variances: {cfg_scale: [-2,4], steps: [-5,5], hr_scale: [-0.50,0.15], hr_enable: false, sampler_name: ['Euler A', 'DPM++ 2M Karras', 'DPM++ 2M']}
#  - llm_param_variances: {repetition_penalty: [-0.25,0.25], temperature_last: false}

  - trigger: 'nude,erotic'
    img_censoring: 0            # 0 = disabled / 1 = blur image / 2 = block image from being generated

  - trigger: 'draw,generate'
    search_mode: user           # Only search user's prompt
    on_prefix_only: true        # Only triggers if matched at very beginning
    should_gen_image: true        # The LLM's reply will be used for Stable Diffusion img generation
    swap_character: Minty-SDXL  # Filename of character in /characters.
    insert_text: ''
    insert_text_method: replace
    load_history: -1            # 0 = default (all history included) / -1 = prompt excludes chat history / > 1 = llm only sees this many recent exchanges.
    save_history: false         # Whether to save this interaction to history or not.

  - trigger: 'take a photo,take {a|another} picture'
    search_mode: user
    should_gen_image: true
    format_prompt: '''[SYSTEM] You have been tasked with creating a text-to-image prompt. Describe the scene in concise and vivid detail: "{prompt}"'''

  - trigger: 'selfie,self portrait'
    search_mode: user
    should_gen_image: true
    format_prompt: '''[SYSTEM] You have been tasked with taking a selfie. Include your appearance, your surroundings, and what you are doing right now: "{prompt}"'''

  - trigger: 'calculate,define'
    on_prefix_only: true
    state: {preset: 'Divine Intellect'} # Update values for textgen-webui state parameters. Must be formatted as a dictionary.

# Example of the 'Flow' tag feature... THE MOST ADVANCED FEATURE IN THIS BOT.
# Creates a message loop, injecting the 'tags' from each flow_step. Paired with the 'format_tag' prompt, the possibilities are ENDLESS (seemingly!)
  - trigger: 'loopback flow'  # Try a prompt such as 'draw a ____________ loopback flow'
    insert_text: ''
    insert_text_method: 'replace'
    flow:
      - flow_step: 1
        flow_step_loops: 5  # Loops this flow step
        format_prompt: 'Provide a more detailed version of this prompt: {llm_0}'
        swap_character: M1nty-SDXL
        should_gen_image: false
      - flow_step: 2
        format_prompt: '{llm_0}'
        swap_character: M1nty-SDXL
        should_gen_image: true

# 'tag_presets' are not in effect by default - they will be imported anywhere you use a 'tag_preset_name' tag.
# Useful for when you want to use the same set of tags for mulitple uses (a few characters, a few imgmodels, etc).
# Example: '- tag_preset_name: SDXL Tags' would be a simple way to share tags among all your SDXL imgmodels defined in dict_imgmodels.yaml.
tag_presets:
  - tag_preset_name: SDXL Turbo Payload   # Each tag preset must be named
    tags:                                 # Tag presets must have their tag definitions nested under a 'tags' key
      - payload: {width: 896, height: 1152, cfg_scale: 2, steps: 5, sampler_name: 'DPM++ SDE Karras'} # Stable Diffusion payload can be modified using 'payload'. Must be formatted as a dictionary.
      #- img_param_variances: {cfg_scale: [-0.5,0.5], steps: [-1,1], hr_scale: [-0.1,0.1], hr_enabled: true}
  
  - tag_preset_name: SDXL Payload
    tags:
      - payload: {width: 896, height: 1152, cfg_scale: 7, steps: 25, sampler_name: 'DPM++ 2M Karras', hr_scale: 1.2}
      #- img_param_variances: {cfg_scale: [-2,3], steps: [0,10], hr_scale: [-0.1,0.1], hr_enabled: true}

  - tag_preset_name: SDXL Tags
    tags:
      - trigger: 'vertical,selfie,self {photo|portrait}'
        payload: {width: 896, height: 1152}
        
      - trigger: 'horizontal,landscape'
        payload: {width: 1152, height: 896}

      - trigger: 'plain'
        payload: {override_settings: {CLIP_stop_at_last_layers: 2}} # Nested payload dictionary settings can also be modified. Must be formatted as a dictionary.

      - trigger: '{no|without|clear|empty|transparent} {bg|background}'
        layerdiffuse: '(SDXL) Only Generate Transparent Image (Attention Injection)'  # enables 'layerdiffuse' extension while applying specified method. See tag references at bottom for list of methods and advanced usage.
      #  Use 'payload' tag if you need more complex triggered settings for layerdiffuse
      #  payload: {'alwayson_scripts': {'layerdiffuse': {'args': [true,'Only Generate Transparent Image (Attention Injection)',0.2,1.0,null,null,null,'Crop and Resize']}}}

      - trigger: 'grimace'
        positive_prompt: '<lora:PE_Grimace_v1.0:0.75>'

      - trigger: '{ps1|playstation} {style|art|graphics},style of {ps1|playstation}'
        positive_prompt: 'ps1 style <lora:ps1_style_SDXL_v2:1.0>'
        negative_prompt: 'blurry' # 'negative_prompt' = inserted at the end of the negative prompt (does not behave like 'positive_prompt' which is relative to the matched text)
        trumps: 'low poly' # If any other matched tag has a trigger phrase matching a 'trump' phrase, it will not be used. (*can be comma separated to trump multiple tags*)

      - trigger: 'low {poly|polygon}'
        positive_prompt: '<lora:LowpolySDXL_v1.0:1.0>'
        negative_prompt: 'blurry'

  - tag_preset_name: SD15 Tags
    tags:
      - payload: {width: 512, height: 512, cfg_scale: 10, steps: 40, sampler_name: 'DPM++ 2M Karras', hr_scale: 2, hr_enabled: true, denoise_strength: 0.5}
      #- img_param_variances: {cfg_scale: [-3,1], steps: [-10,10], hr_scale: [-0.5,0.2]}

      - positive_prompt_prefix: 'masterpiece, detailed, ' # '_prefix' makes this inserted at the beginning of the image prompt
        positive_prompt_suffix: ' <lora:more_details:0.5> <lora:epiNoiseoffset_v2Pynoise:0.5>' # '_suffix' makes this inserted at the end of the image prompt
        negative_prompt_prefix: '<kkw-Extreme-Neg:0.5>, <negative_hand-neg:0.5>, by <bad-artist:0.5>, ' # same logic as positive_prompt. Can similarly use '_suffix' (or just 'negative_prompt')

      - trigger: 'selfie,self photo'
        positive_prompt: '(taking a selfie:1.2), (arms outstretched:1.1)'

      - trigger: 'gta,gta4,grand theft auto,archer style'
        positive_prompt: 'GtaSA2004 cartoon <lora:GTA_Style:1.0>'

      - trigger: 'gigachad'
        positive_prompt: '<lora:Gigachadv1:0.8>'


# List of all available tag keys:
#
# Relevant to all tags:
#   search_mode: userllm              Search method for trigger matching. 'user' = user prompt only / 'llm' = bot reply only / 'userllm' = search both for matches
#   trigger: 'trigger,another one'    Recommended for most tags (except 'global_tag_keys' and 'tag_preset_name').
#   random: 0.5                       Chance for tag to process. 0.0 = 0% chance and 1.0 = 100% chance.
#   case_sensitive: (true/false)      If the triggers are case sensitive
#   on_prefix_only: (true/false)      If triggers must be matched at beginning of the searched text

# Most advanced tag (Relevant to all):
#   flow: {list}                      A list of 'flow_steps'. When a 'flow' tag is triggered, it creates a message loop for each defined 'loop_step'
#   flow_step: {dict}                 The tags you want to apply in each flow step.  Without a trigger, they will auto-apply.  'format_prompt' is very useful in each flow_step
#   flow_step_loops: int              The number of times you want to loop a particular flow_step.

# Relevant to text generation (LLM):
#   should_gen_text: (true/false)     If LLM should generate text. If False and 'should_gen_image' is True, passes the user's text as prompt for image gen.
#   should_send_text: (true/false)    If LLM should send generated text to channel. Mainly used for 'flows' tag.
#   change_llmmodel: 'string'         For (change/swap)_llmmodel: 'change' will trigger a persistent change. 'swap' will only change the LLM model for the current interaction.
#   swap_llmmodel: 'string'           Name of an LLM model (ei: 'Mistral-7B-OpenOrca-GPTQ'). Value 'None' will unload the LLM model!
#   time_offset: 0.0                  0 = today's date (system time). -0.5 shifts the current date to be 12 hours ago. (UOM = days). Use with '{time}' anywhere in your prompts
#   time_format: '%H:%M:%S'           Examples: '%H:%M:%S' (HH:MM:SS), '%I:%M:%S %p' (12HR:MM:SS AM/PM)
#   date_format: '%Y-%m-%d'           Examples: '%Y-%m-%d' (YYYY-MM-DD), '%A, %B %d, %Y' (DayName, MonthName DD, YYYY)
#   format_prompt: 'string'           Useful for adding text before and/or after your prompt to the LLM. **If you do not include {prompt}, your prompt will be replaced entirely with the value of 'format_prompt'.** Can include any variables, see '/tips_and_info/Message Variables.txt'
#   insert_text: ''                   Text you may want to insert relative to the matched search text.
#   insert_text_method: replace       Relevant in tags with key 'pinsert_text'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text
#   text_joining: ' '                 Relevant to all insertion methods except "replace"
#   change_character: 'string'        Filename of character in /characters (do not include extension). That character's name, context, and state parameters will be used in LLM payload.
#   swap_character: 'string'          For (change/swap)_character: 'change' will trigger a persistent change. 'swap' will only change the character for the current interaction.
#   load_history: 0                   0 = default (all history included) / -1 = prompt excludes chat history / > 1 = llm only sees this many recent exchanges.
#   save_history: (true/false)        Whether to save this interaction to history or not.
#   state: {dictionary}               Update values for textgen-webui state parameters. Must be formatted as a dictionary.
#   llm_param_variances: {dictionary} Randomization ranges for textgen-webui state parameters. Must be formatted as a dictionary.

# Relevant to img generation (A1111 / Forge):
#   should_gen_image: (true/false)    If bot should generate an image using Stable Diffusion. The LLM's reply will be used as the img prompt, unless no LLM Model is loaded or 'should_gen_text' is False - in which case user message is prompt.
#   should_send_image: (true/false)   If bot should send generated image to channel. Mainly used for 'flows' tag.
#   change_imgmodel: 'string'         For (change/swap)_imgmodel: 'change' will trigger a persistent change. 'swap' will only change the Img model for the current interaction.
#   swap_imgmodel: 'string'           Copy/paste a checkpoint name *exactly* as it appears in A1111/Forge model list (ei: 'sdxl\epicrealismXL_v4Photoreal.safetensors [c772a1a690]').
#                                     Currently, API Img model unloading is bugged in both A1111/Forge so that is not an option here.
#   img_censoring: 0                  0 = disabled / 1 = blur image / 2 = block image from being generated
#   positive_prompt: 'string'         Text you may want to insert relative to the matched text in the image prompt.
#   positive_prompt_method: after     Relevant to 'positive_prompt'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text
#   positive_prompt_prefix: 'string'  Insert text at the beginning of the image positive prompt
#   positive_prompt_suffix: 'string'  Insert text at the end of the image positive prompt
#   negative_prompt_prefix: 'string'  Insert text at the beginning of the image negative prompt
#   negative_prompt: 'string'         Insert text at the end of the image negative prompt ('_suffix' also works)
#   img_text_joining: ', '            Relevant to all insertion methods except "replace"
#   payload: {dictionary}             Replacements for Stable Diffusion payload. Must be formatted as a dictionary.
#   img_param_variances: {dictionary} Randomization ranges for Stable Diffusion payload parameters. Must be formatted as a dictionary.


# HINT: tags can be created/activated on-demand if using this sytax in your prompt: [[key:value]] or [[key1:value1 | key2:value2]] etc
#       EXAMPLE (Single tag):     [[reactor_max_faces:6]]
#       EXAMPLE (Multiple tags):  [[reactor_save_original:True | laydiff_weight:0.5]]
#   ** This is a good way to add specific extension parameters on-demand!! **

# ControlNet specific tags (A1111/Forge extension):
#   controlnet: 'string'              Simplified tag to both **apply** ControlNet image/folder AND **enable** ControlNet.
#   controlnet#: 'string'             Optional syntax for multi-ControlNet where # represents the ControlNet unit. ex: 'controlnet0: guy.png', 'controlnet1: architecture', etc.
#                                     Value values: Image in 'controlnet_images' (png/jpg/txt in base64) -OR- Directory name in '/controlnet_images/' (Picks a random image)
#   cnet_{key}                        ANY key can be used (see dict_base_settings.yaml for complete list). A 'controlnet' tag must also be in effect when using these tags in order to properly assign the image.
#   cnet#_{key}: 'string'             Optional syntax for multi-ControlNet parameters where # represents the ControlNet unit. ex: 'cnet0_weight: 0.5', 'cnet1_guidance_end: 0.8', etc.
#   cnet#_mask_image: 'string'        The value for the 'cnet_mask_image' param will be handled exactly the same as the main controlnet (see writeup for 'controlnet' a few lines up)

# layerdiffuse specific tags (Forge extension):
#   layerdiffuse: 'string'            Simplified tag to both **apply** AND **enable** layerdiffuse. Note: 'laydiffuse_method' works exactly the same.
#   laydiff_method: 'string'          Valid values for either 'layerdiffuse' or 'laydiff_method':
#                                         "(SD1.5) Only Generate Transparent Image (Attention Injection)"
#                                         "(SD1.5) From Foreground to Background (need batch size 2)"
#                                         "(SD1.5) From Background to Foreground (need batch size 2)"
#                                         "(SD1.5) Generate Everything Together (need batch size 3)"
#                                         "(SDXL) Only Generate Transparent Image (Attention Injection)"
#                                         "(SDXL) Only Generate Transparent Image (Conv Injection)"
#                                         "(SDXL) From Foreground to Blending"
#                                         "(SDXL) From Foreground and Blending to Background"
#                                         "(SDXL) From Background to Blending"
#                                         "(SDXL) From Background and Blending to Foreground"
#   laydiff_{key}                     ANY key can be used (see dict_base_settings.yaml for complete list). Note: layerdiffusion will not be enabled without either a 'layerdiffuse' or 'laydiff_method' tag.

# ReActor specific tags (A1111/Forge extension):
#   reactor: 'string'                 Simplified tag to both **apply** face image/folder/model AND **enable** ReActor.
#                                     Value values: Image in 'swap_faces' (png/jpg/txt in base64) -OR- Directory name in '/swap_faces/' (Picks a random face) -OR- Face model in '{SD_CLIENT}\models\reactor\faces' example 'Minty.safetensors'
#   reactor_{key}                     ANY key can be used (see dict_base_settings.yaml for complete list).
#                                     Recommended to use in combination with 'reactor' tag - otherwise, you can't use "single face image method"
#                                     (technically, you could send a base64 string... and you would need to include 'reactor_enabled: True')