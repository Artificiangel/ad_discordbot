# Options for /llmstates command
# Each character can have its own llmstate defined in it's char file (see example characters)

# **If character file does not have a defined llmstate, 'Default' will be applied**
- llmstate_name: Default
  state:
    preset: 'Midnight Enigma'
    max_new_tokens: 400
    max_tokens_second: 0
    seed: -1.0
    temperature: 0.98
    temperature_last: false
    top_p: 0.37
    top_k: 100
    tfs: 0
    top_a: 0
    typical_p: 1
    epsilon_cutoff: 0
    eta_cutoff: 0
    frequency_penalty: 0
    presence_penalty: 0
    repetition_penalty: 1.18
    repetition_penalty_range: 0
    encoder_repetition_penalty: 1
    no_repeat_ngram_size: 0
    min_length: 50
    do_sample: true
    penalty_alpha: 0
    num_beams: 1
    length_penalty: 1
    early_stopping: false
    add_bos_token: true
    ban_eos_token: false
    skip_special_tokens: true
    truncation_length: 2048
    custom_stopping_strings: '"### Assistant","### Human","</END>","\nclient.user.display_name"'
    auto_max_new_tokens: false
    name1: ''
    name2: client.user.display_name
    name1_instruct: ''
    name2_instruct: client.user.display_name
    turn_template: ''
    chat_prompt_size: 2048
    chat_generation_attempts: 1
    stop_at_newline: false
    mode: chat
    stream: true
    mirostat_mode: 0
    mirostat_tau: 5.00
    mirostat_eta: 0.10
    grammar_string: ''
    guidance_scale: 1
    negative_prompt: ''
    end_of_turn: ''

- llmstate_name: Img Mode
  state:
    preset: 'Midnight Enigma'
    max_new_tokens: 100
    max_tokens_second: 0
    seed: -1.0
    temperature: 0.98
    top_p: 0.37
    top_k: 100
    tfs: 0
    top_a: 0
    typical_p: 1
    epsilon_cutoff: 0
    eta_cutoff: 0
    repetition_penalty: 1.18
    repetition_penalty_range: 0
    encoder_repetition_penalty: 1
    no_repeat_ngram_size: 0
    min_length: 50
    do_sample: true
    truncation_length: 2048
    chat_prompt_size: 2048
    mode: chat

- llmstate_name: Chat Mode
  state:
    preset: ''
    max_new_tokens: 400
    max_tokens_second: 0
    seed: -1.0
    temperature: 0.98
    top_p: 0.37
    top_k: 100
    tfs: 0
    top_a: 0
    typical_p: 1
    epsilon_cutoff: 0
    eta_cutoff: 0
    repetition_penalty: 1.18
    repetition_penalty_range: 0
    encoder_repetition_penalty: 1
    no_repeat_ngram_size: 0
    min_length: 50
    do_sample: true
    truncation_length: 2048
    chat_prompt_size: 2048
    mode: chat

- llmstate_name: Chat Long Context
  state:
    preset: ''
    max_new_tokens: 500
    max_tokens_second: 0
    seed: -1.0
    temperature: 0.98
    top_p: 0.37
    top_k: 100
    tfs: 0
    top_a: 0
    typical_p: 1
    epsilon_cutoff: 0
    eta_cutoff: 0
    repetition_penalty: 1.18
    repetition_penalty_range: 0
    encoder_repetition_penalty: 1
    no_repeat_ngram_size: 0
    min_length: 50
    do_sample: true
    truncation_length: 4096
    chat_prompt_size: 4096
    mode: chat

- llmstate_name: Instruct Mode
  state:
    preset: ''
    mode: instruct